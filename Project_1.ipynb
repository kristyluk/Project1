{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "#!pip install mysql\n",
    "!pip3 install mysql-connector-python-rf\n",
    "import mysql.connector\n",
    "from pprint import pprint\n",
    "import pymongo\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b78b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file system using CSV\n",
    "data = pd.read_csv(\"products.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0723799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming existing data by deleting NA values\n",
    "\n",
    "new_data = data.dropna()\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640717ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB\n",
    "# declaring and assigning connection variables\n",
    "mysql_args = {\n",
    "    \"uid\" : \"root\",\n",
    "    \"pwd\" : \"1014810473\",\n",
    "    \"hostname\" : \"localhost\",\n",
    "    \"dbname\" : \"adventureworks_dw\"\n",
    "}\n",
    "\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"uec6ct\",\n",
    "    \"password\" : \"RNu3n7QuGWV1FOg3\",\n",
    "    \"cluster_name\" : \"cluster0\",\n",
    "    \"cluster_subnet\" : \"y4eygtf\",\n",
    "    \"cluster_location\" : \"atlas\", \n",
    "    \"db_name\" : \"adventureworks_dw\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions for getting data from and setting data into databases\n",
    "\n",
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        connection.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the cluster_location parameter.\")\n",
    "    \n",
    "    else:\n",
    "        if args[\"cluster_location\"] == \"atlas\":\n",
    "            connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "            connect_str += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "            client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "            \n",
    "        elif args[\"cluster_location\"] == \"local\":\n",
    "            client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        \n",
    "    return client\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_mongo_collections(mongo_client, db_name, data_directory, json_files):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ff384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating MongoDB with source data\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "json_files = {\"purchase\" : 'purchase.json'\n",
    "             }\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1aa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from the source MongoDB into purchase dataframe\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "query = {} \n",
    "collection = \"purchase\"\n",
    "\n",
    "df_purchase = get_mongo_dataframe(client, mongodb_args[\"db_name\"], collection, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = \"SELECT date_key, full_date FROM adventureworks_dw.dim_date;\"\n",
    "df_dim_date = get_sql_dataframe(sql_dim_date, **mysql_args)\n",
    "df_dim_date.full_date = df_dim_date.full_date.astype('datetime64[ns]').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b87ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_DueDate = df_dim_date.rename(columns={\"date_key\" : \"DueDate_key\", \"full_date\" : \"DueDate \"})\n",
    "df_DueDates.DueDate  = df_DueDates.DueDate.astype('datetime64[ns]').dt.date\n",
    "df_DueDates = pd.merge(df_DueDates, df_dim_DueDate, on='DueDate', how='left')\n",
    "df_DueDates.drop(['DueDate '], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to dataframes\n",
    "\n",
    "df_DueDates.rename(columns={\"id\":\"duedate_id\"}, inplace=True)\n",
    "\n",
    "df_DueDates.drop(['due_date'], axis=1, inplace=True)\n",
    "\n",
    "df_DueDates.insert(0, \"DueDate_key\", range(1, df_DueDates.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading transford dataframes into new data warehouse\n",
    "\n",
    "dataframe = df_DueDates\n",
    "table_name = 'dim_DueDates'\n",
    "primary_key = 'DueDate_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
